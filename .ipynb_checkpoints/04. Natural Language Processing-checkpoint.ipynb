{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d473958",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5ff73",
   "metadata": {},
   "source": [
    "We starten weer met het laden van de benodigde package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a90fad",
   "metadata": {},
   "source": [
    "En het inladen van onze gefilterde dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452108fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fiets.csv', index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106225fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef5f99",
   "metadata": {},
   "source": [
    "## Woord frequenties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82527921",
   "metadata": {},
   "source": [
    "Als eerste, meeste simpele stap van tekst analyse, kunnen we kijken naar de meest voorkomende woorden in de artikelen. Onderstaande functie is geschreven om dit te doen. \n",
    "\n",
    "Maar voordat we deze functie kunnen gebruiken moeten we de artikelen van de kolom `content` omzetten naar een lijst met woorden per artikel. \n",
    "Dit doen we met de `split` functie. Omdat we voor het tellen geen rekening willen houden met hoofdletters, zetten we alle letters om naar kleine letters. Dit doen we met de functie `lower`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_list'] = data.content.apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1681788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(dataframe_column):\n",
    "    full_list = []\n",
    "    for elemnt in dataframe_column:\n",
    "        full_list += elemnt\n",
    "    \n",
    "    values_count = pd.Series(full_list).value_counts()\n",
    "    return values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f353d3",
   "metadata": {},
   "source": [
    "Nu we de kolom `content_list` hebben, kunen we de `word_counter` functie uitproberen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter(data['content_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef88c8",
   "metadata": {},
   "source": [
    "Zoals je ziet zijn de eerste woorden allemaal woorden die inhoudelijk niet veel zeggen over de artikelen. De meest voorkomende woorden in teksten zijn vaak lidwoorden, voegwoorden, voorzetsels en dergelijke. Dit zijn woorden die vaak weinig informatie toevoegen over de inhoud van teksten. In Natural Language Processing noemen we deze woorden `stopwoorden`.  Afhankelijk van de analyse en de dataset kunnen de gebruikte stopwoorden veranderen. Voor Nederlands zijn veelgebruikte stopwoorden de lidwoorden, voegwoorden en voornaamwoorden. Ook artefacten uit de tekst kunnen worden toegevoegd. \n",
    "\n",
    "In onze datafolder staat een stopwoordenlijst klaar die we kunnen gebruiken voor het uitfilteren van deze woorden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords-nl.txt', 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328f263",
   "metadata": {},
   "source": [
    "En hieronder staat een nieuwe functie, die deze woorden meeneemt als filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter_stopword(dataframe_column):\n",
    "    full_list = []\n",
    "    for elemnt in dataframe_column:\n",
    "        full_list += elemnt\n",
    "        \n",
    "    full_list = [i for i in full_list if i not in stopwords]\n",
    "    \n",
    "    values_count = pd.Series(full_list).value_counts()\n",
    "    return values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cc86a",
   "metadata": {},
   "source": [
    "Nu kunnen we onze nieuwe functie `word_counter_stopword` draaien over de kolom `content_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948169e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter_stopword(data['content_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4cfa7",
   "metadata": {},
   "source": [
    "Dit ziet er al een stuk beter uit. Met behulp van de `head` functie kunnen we de top 10 tonen. Zoals je ziet is het nog niet helemaal perfect, want het meest voorkomende teken is een streepje, en er staat nog een komma tussen. Normaliter schoon je de tekst dus nog wat verder op voordat je er mee verder gaat. Omdat dit puur ter demonstratie is slaan we deze stap nu over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter_stopword(data['content_list']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b77af",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd0aa6",
   "metadata": {},
   "source": [
    "Naast een naieve wordcount, zijn er ook andere opties mogelijk. Voor het vervolg van deze workshop gaan we gebruik maken van de Spacy package. Deze package wordt veel gebruikt wordt voor Natural Language Processing. Een andere veelgebruikte package is `NLTK`. \n",
    "\n",
    "Voor het vervolg van dit Notebook hebben we aantal packages nodig. Dus we starten met het importeren van deze packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aebeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac41b3f",
   "metadata": {},
   "source": [
    "Als je Spacy vraagt om Natural Language Processing uit te voeren, worden voor elke input verschillende Natural Language Processing taken uitgevoerd, waaronder het herkennnen van entities en woordsoorten. Deze informatie wordt opgeslagen als een `doc` item. Het draaien van deze code duurt enige tijd. We hebben daarom het model alvast voorbereid en klaargezet in de map `data`. De code voor het draaien van een model staat hieronder als voorbeeld aangegeven. Dit kun je gebruiken als je je eigen  model wilt draaien op bijvoorbeeld een andere dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a068d15",
   "metadata": {},
   "source": [
    "```\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "data = data.dropna(subset=['content'])\n",
    "\n",
    "def process_text(text):\n",
    "   return nlp(text)\n",
    "   \n",
    "data[\"doc\"] = data[\"content\"].apply(process_text)\n",
    "\n",
    "with open('data/fiets_nlp.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c4c39",
   "metadata": {},
   "source": [
    "Voor het vervolg van dit Notebook laden we het model dat in de `data` map staat. Dit doen we met de volgende code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiets_nlp = pd.read_pickle(\"data/fiets_nlp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiets_nlp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382f577",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bde250",
   "metadata": {},
   "source": [
    "Spacy slaat informatie over de gevonden named entities op in het doc item. Deze informatie staat opgeslagen in de kolom `doc` en kan daar ook uitgehaald worden.\n",
    "\n",
    "Spacy kent de volgende Named entities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5afebe",
   "metadata": {},
   "source": [
    "* PERSON:      People, including fictional.\n",
    "* NORP:        Nationalities or religious or political groups.\n",
    "* FAC:         Buildings, airports, highways, bridges, etc.\n",
    "* ORG:         Companies, agencies, institutions, etc.\n",
    "* GPE:         Countries, cities, states.\n",
    "* LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART: Titles of books, songs, etc.\n",
    "* LAW:         Named documents made into laws.\n",
    "* LANGUAGE:    Any named language.\n",
    "* DATE:        Absolute or relative dates or periods.\n",
    "* TIME:        Times smaller than a day.\n",
    "* PERCENT:     Percentage, including ”%“.\n",
    "* MONEY:       Monetary values, including unit.\n",
    "* QUANTITY:    Measurements, as of weight or distance.\n",
    "* ORDINAL:     “first”, “second”, etc.\n",
    "* CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94325f",
   "metadata": {},
   "source": [
    "Je kan ook de spacy module vragen voor uitleg over een bepaalde Named Entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1df4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94caf40",
   "metadata": {},
   "source": [
    "Je kan per artikel de gevonden Named Entities tonen. Deze worden dan met verschillende kleuren in de tekst weergegeven.\n",
    "\n",
    "Eerst maken we een variabele `doc` aan, waarin we de inhoud van de kolom `doc` voor één artikel stoppen. Dit doen we met de blokhaken `[2]`. In dit geval roepen we het derde artikel aan (Python telling begint bij 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c99289",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fiets_nlp['doc'].to_list()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142df8b0",
   "metadata": {},
   "source": [
    "Vervolgens kunnen we met de functie `displacy.render()` dit artikel inclusief de bijbehorende Named Entities tonen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e01b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb9d89",
   "metadata": {},
   "source": [
    "### **Oefening**\n",
    "We hebben net het derde artikel bekeken. Kies nu zelf een ander artikel om te bekijken. \n",
    "Maak eerst een variabel aan voor dit artikel, en roep dan de functie `displacy.render()` aan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f35e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maak een nieuwe variabele met de content van één artikel\n",
    "## Voorbeeld syntax: variabele_naam = fiets_nlp['doc'].to_list[nummer van het artikel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2722b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gebruik displacy.render() om het artikel te tonen\n",
    "## Voorbeeld syntax: displacy.render(variable_naam, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56712d",
   "metadata": {},
   "source": [
    "Naast de Named Entities op deze manier tonen, kun je er ook verdere analyses mee doen. Je kan ze bijvoorbeeld per category opslaan als losse kolommen in je dataframe en deze kolommen gebruiken om de meest voorkomende entities per categorie te tonen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4fe58",
   "metadata": {},
   "source": [
    "Hieronder staat een functie waarmee je Named Entities van een bepaalde soort uit de `doc` kolom kan halen. Deze functie kan je gebruiken om de gevonden entities als een lijst op te slaan in een nieuwe kolom in het dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47426ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(doc, entity):\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == entity]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9713b",
   "metadata": {},
   "source": [
    "In het onderstaande voorbeeld wordt de category `GPE` uit het`doc` item gehaald. Deze informatie wordt vervolgens opgeslagen in de kolom `GPE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiets_nlp['GPE'] = fiets_nlp['doc'].apply(lambda x: get_ner(x, 'GPE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8df4a",
   "metadata": {},
   "source": [
    "Vervolgens kun je deze nieuwe kolom weer gebruiken om een top 10 van de meest voorkomende GPE locaties uit de dataset te halen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter(fiets_nlp['GPE']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee9576",
   "metadata": {},
   "source": [
    "### **Oefening** \n",
    "Maak nu zelf een kolom aan voor een andere Named Entity naar keuze, en bekijk de top 10. Kies een entity uit de lijst hierboven. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece31fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stap 1: maak een nieuwe kolom aan voor de gekozen entity\n",
    "## Voorbeeld syntax: fiets_nlp['naam nieuwe kolom'] = fiets_nlp['doc'].apply(lambda x: get_ner(x, 'naam entity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stap 2: Toon de tien meest voorkomende woorden van je gekozen category.\n",
    "## Word counter: word_counter(fiets_nlp['nieuwe kolom']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5a17a",
   "metadata": {},
   "source": [
    "### **Bonus oefening**\n",
    "Doe hetzelfde voor enkele andere Named Entities. Kun je ook een top 15 of 25 maken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schrijf hier de code voor de bonus oefening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b200c",
   "metadata": {},
   "source": [
    "Hierboven hebben we telkens de top 10 van een Named Entity op de hele dataset getoond. Je kunt de dataset ook splitsen, en dan de top 10 van de verschillende subframes vergelijken. \n",
    "\n",
    "We geven hieronder een voorbeeld voor de Named Entity `GPE` en we splitsen de dataframe op basis van de kolom `spatial`. \n",
    "\n",
    "We beginnen  met het maken van twee subdataframes: `landelijk_df` en `lokaal_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "landelijk_df = fiets_nlp[fiets_nlp['spatial'] == 'Landelijk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionaal_df = fiets_nlp[fiets_nlp['spatial'] == 'Regionaal/lokaal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96e81c",
   "metadata": {},
   "source": [
    "Nu kunnen we deze subframes gebruiken voor een top 10 per frame, waarna we deze kunnen vergelijken met elkaar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter(landelijk_df['GPE']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b78997",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter(regionaal_df['GPE']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d779386",
   "metadata": {},
   "source": [
    "### **Oefening**\n",
    "Gebruik de subframes `landelijk_df` en `regionaal_df` om de meest voorkomenden woorden van jouw eigen hierboven gekozen entity te vergelijken. Gebruik hiervoor de kolom die bij de oefening hierboven zelf hebt aangemaakt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Voorbeel syntax: word_counter(landelijk_df['kolomnaam eigen entity']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a42db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Voorbeel syntax: word_counter(regionaal_df['kolomnaam eigen entity']).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
